{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import functools as fn\n",
    "from functools import wraps\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape : (60000, 784)\n",
      "Y shape : (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "mat = scipy.io.loadmat('data_image_classif.mat')\n",
    "\n",
    "X = mat['Xts']\n",
    "n = X.shape[1]\n",
    "\n",
    "X[X != 0] = 1\n",
    "\n",
    "df_y = pd.DataFrame(mat['yts'])\n",
    "Y = pd.get_dummies(df_y[0]).to_numpy() # OneHotEncoding\n",
    "print(f'X shape : {X.shape}')\n",
    "print(f'Y shape : {Y.shape}')\n",
    "\n",
    "X_to_predict = mat['Xvr']\n",
    "X_to_predict[X_to_predict != 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "test_size = int(0.3 * len(X)) \n",
    "X, Y = unison_shuffled_copies(X, Y)\n",
    "X_train, Y_train = X[test_size:], Y[test_size:]\n",
    "X_test, Y_test = X[:test_size], Y[:test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeit(func):\n",
    "    @wraps(func)\n",
    "    def timeit_wrapper(*args, **kwargs):\n",
    "        start_time = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.perf_counter()\n",
    "        total_time = end_time - start_time\n",
    "        print(f'Function {func.__name__} elapsed {total_time:.2e}s')\n",
    "        return result\n",
    "    return timeit_wrapper\n",
    "\n",
    "def plot(x, y, title):\n",
    "    ax = plt.axes()\n",
    "    ax.plot(x, y, c='r', lw=1)\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title(title)\n",
    "    # ax.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, n) -> None:\n",
    "        self.n = n\n",
    "        self.m = 10\n",
    "        self.mn = self.n * self.m\n",
    "        self.w = np.random.uniform(-0.05, 0.05, (self.m * (self.n + 1), 1))\n",
    "        self.gradw = np.zeros_like(self.w)\n",
    "        self.update_params()\n",
    "\n",
    "    def reset(self):\n",
    "        self.w = np.random.uniform(-0.05, 0.05, (self.m * (self.n + 1), 1))\n",
    "        self.update_params()\n",
    "\n",
    "    def update_params(self):\n",
    "        self.W = self.w[:self.n * self.m].reshape((self.m, self.n))\n",
    "        self.b = self.w[self.n * self.m:].reshape((self.m, 1))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        z = self.W@x + self.b\n",
    "        e_z = np.exp(z - np.max(z))\n",
    "        act_func = e_z / e_z.sum(axis=0)\n",
    "        return act_func\n",
    "\n",
    "    def compute_single_gradient(self, x, y):\n",
    "        gradw = np.zeros((self.m * (self.n + 1), 1))\n",
    "        y_diff = self.softmax(x) - y\n",
    "\n",
    "        dW = y_diff @ x.T\n",
    "        gradw[:self.mn, :] = dW.reshape((self.mn, 1))\n",
    "        gradw[self.mn:, :] = y_diff\n",
    "        return gradw\n",
    "\n",
    "    def compute_gradient(self, X, Y, single=False):\n",
    "        gradw = np.zeros_like(self.w)\n",
    "\n",
    "        if single:\n",
    "            gradw = self.compute_single_gradient(X.reshape(self.n, 1), Y.reshape(self.m, 1))\n",
    "        else:\n",
    "            for i in range(X.shape[0]):\n",
    "                gradw += self.compute_single_gradient(X[i, :].reshape(self.n, 1), Y[i, :].reshape(self.m, 1))\n",
    "            gradw /= X.shape[0]\n",
    "        return gradw\n",
    "\n",
    "    def calculate_loss(self, X, Y):\n",
    "        loss = 0\n",
    "        for i in range(X.shape[0]):\n",
    "            x = X[i, :].reshape((self.n, 1))\n",
    "            y = Y[i, :].reshape((self.m, 1))\n",
    "\n",
    "            z = self.softmax(x)\n",
    "            loss -= np.log(z[np.argmax(y)])\n",
    "        return loss\n",
    "\n",
    "    def train(self, X, Y, trainer, steps, do_plot=False):\n",
    "        self.reset()\n",
    "        loss = [0] * steps\n",
    "        for i in tqdm(range(steps)):\n",
    "            trainer(X, Y)\n",
    "            if do_plot:\n",
    "                loss[i] = self.calculate_loss(X, Y)\n",
    "\n",
    "        if do_plot:\n",
    "            plot([i for i in range(1, steps+1)], loss, 'Loss during train')\n",
    "        return loss\n",
    "\n",
    "    def test(self, X, Y):\n",
    "        result = 0\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            x = X[i, :].reshape((self.n, 1))\n",
    "            y = Y[i, :].reshape((self.m, 1))\n",
    "            \n",
    "            act_func = self.softmax(x)\n",
    "\n",
    "            y_pred = np.argmax(act_func)\n",
    "            y_real = np.argmax(y)\n",
    "\n",
    "            if y_pred == y_real:\n",
    "                result += 1\n",
    "        \n",
    "        accuracy = round(result / len(X) * 100, 1)\n",
    "        return accuracy\n",
    "    \n",
    "    def full_gd(self, X: np.ndarray, Y: np.ndarray, params):\n",
    "        lr = params[0]\n",
    "\n",
    "        self.w = self.w - lr * self.compute_gradient(X, Y)\n",
    "        self.update_params()\n",
    "\n",
    "    def sgd(self, X: np.ndarray, Y: np.ndarray, params):\n",
    "        selected = np.random.randint(0, self.n)\n",
    "        lr = params[0]\n",
    "\n",
    "        self.w = self.w - lr * self.compute_gradient(X[selected, :], Y[selected, :], single=True)\n",
    "        self.update_params()\n",
    "\n",
    "    def batch_sgd(self, X: np.ndarray, Y: np.ndarray, params):\n",
    "        lr = params[0] \n",
    "        batch_size = params[1]\n",
    "\n",
    "        selected = np.random.randint(0, self.n, size=batch_size)\n",
    "        self.w = self.w - lr * self.compute_gradient(X[selected, :], Y[selected, :])\n",
    "        self.update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [44:23<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 91.1%\n"
     ]
    }
   ],
   "source": [
    "# Full GD\n",
    "lr = 0.2\n",
    "steps = 2000\n",
    "do_plot = False\n",
    "model_full = Perceptron(n)\n",
    "loss_full = model_full.train(X_train, Y_train, fn.partial(model_full.full_gd, params=[lr]), steps, do_plot)\n",
    "\n",
    "acc = model_full.test(X_test, Y_test)\n",
    "print(f'Model accuracy: {acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD\n",
    "lr = 0.2\n",
    "steps = 1000\n",
    "do_plot = True\n",
    "model_sgd = Perceptron(n)\n",
    "loss_sgd = model_sgd.train(X_train, Y_train, fn.partial(model_sgd.sgd, params=[lr]), steps, do_plot)\n",
    "\n",
    "acc = model_sgd.test(X_test, Y_test)\n",
    "print(f'Model accuracy: {acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch SGD\n",
    "lr = 0.2\n",
    "batch_size = 1000\n",
    "steps = 1000\n",
    "do_plot = True\n",
    "model_batch = Perceptron(n)\n",
    "loss_batch = model_batch.train(X_train, Y_train, fn.partial(model_batch.batch_sgd, params=[lr, batch_size]), steps, do_plot)\n",
    "# model_batch.train(X, Y, fn.partial(model_batch.batch_sgd, params=[lr, batch_size]), steps=1000)\n",
    "\n",
    "acc = model_batch.test(X_test, Y_test)\n",
    "print(f'Model accuracy: {acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model: Perceptron, X):\n",
    "    size = X.shape[0]\n",
    "    Y = [0] * size\n",
    "\n",
    "    for i in range(size):\n",
    "        x = X[i, :].reshape((model.n, 1))\n",
    "        act_func = model.softmax(x)\n",
    "        y_pred = np.argmax(act_func)\n",
    "        Y[i] = (y_pred + 1) * 100 + 1\n",
    "\n",
    "    result = pd.DataFrame({'id': [i for i in range(1, size + 1)], 'class': Y})\n",
    "    result.to_csv(\"result.csv\", sep=',', index=False)\n",
    "\n",
    "predict(model_full, X_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
